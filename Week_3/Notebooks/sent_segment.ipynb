{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Article 1\n",
    "article_1 = \"\"\"Technology is evolving rapidly; every day, we witness new advancements in artificial intelligence, robotics, and data science. \n",
    "The question is: are we ready for such changes? Many experts believe that automation will replace millions of jobsâ€”some say this is a threat, while others see it as an opportunity! \n",
    "Nevertheless, innovation continues: new industries are emerging, and with them, new career paths are being created.\"\"\"\n",
    "\n",
    "# English Article 2\n",
    "article_2 = \"\"\"History has taught us valuable lessonsâ€”some we remember, others we repeat! \n",
    "Take, for example, the rise and fall of ancient civilizations: the Roman Empire, the Maya, and even the Egyptian dynasties. \n",
    "What led to their decline? Was it internal corruption, external invasion, or simply the passage of time? \n",
    "Regardless, one thing remains certain: no empire lasts forever!\"\"\"\n",
    "\n",
    "# English Article 3\n",
    "english_pipline_test = \"\"\"Artificial intelligence is rapidly transforming various industries; businesses are adopting machine learning models to automate processes, \n",
    "enhance customer experiences, and gain insights from data; however, ethical concerns regarding data privacy, algorithmic bias, and job displacement continue\n",
    "to be major topics of discussion in academic and professional circles worldwide.\"\"\"\n",
    "\n",
    "# Arabic Article 1\n",
    "article_3 = \"\"\"Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ØªØªØ·ÙˆØ± Ø¨Ø³Ø±Ø¹Ø©Ø› Ù†Ø´Ù‡Ø¯ ÙƒÙ„ ÙŠÙˆÙ… ØªØ·ÙˆØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙˆØ§Ù„Ø±ÙˆØ¨ÙˆØªØ§ØªØŒ ÙˆØ¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. \n",
    "Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‡Ùˆ: Ù‡Ù„ Ù†Ø­Ù† Ù…Ø³ØªØ¹Ø¯ÙˆÙ† Ù„Ù‡Ø°Ù‡ Ø§Ù„ØªØºÙŠØ±Ø§ØªØŸ ÙŠØ¹ØªÙ‚Ø¯ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø£Ù† Ø§Ù„Ø£ØªÙ…ØªØ© Ø³ØªØ­Ù„ Ù…Ø­Ù„ Ù…Ù„Ø§ÙŠÙŠÙ† Ø§Ù„ÙˆØ¸Ø§Ø¦Ùâ€”Ø§Ù„Ø¨Ø¹Ø¶ ÙŠØ±Ø§Ù‡Ø§ ØªÙ‡Ø¯ÙŠØ¯Ù‹Ø§ØŒ Ø¨ÙŠÙ†Ù…Ø§ ÙŠØ±Ø§Ù‡Ø§ Ø¢Ø®Ø±ÙˆÙ† ÙØ±ØµØ©! \n",
    "ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ØªØ³ØªÙ…Ø± Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª: ØªÙ†Ø´Ø£ ØµÙ†Ø§Ø¹Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙˆÙ…Ø¹Ù‡Ø§ ØªÙØ®Ù„Ù‚ Ù…Ø³Ø§Ø±Ø§Øª ÙˆØ¸ÙŠÙÙŠØ© Ø­Ø¯ÙŠØ«Ø©.\"\"\"\n",
    "\n",
    "# Arabic Article 2\n",
    "article_4 = \"\"\"Ø¹Ù„Ù…ØªÙ†Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¯Ø±ÙˆØ³Ù‹Ø§ Ù‚ÙŠÙ‘Ù…Ø©â€”Ø¨Ø¹Ø¶Ù‡Ø§ Ù†ØªØ°ÙƒØ±Ù‡ØŒ ÙˆØ¨Ø¹Ø¶Ù‡Ø§ Ù†Ø¹ÙŠØ¯ ØªÙƒØ±Ø§Ø±Ù‡! \n",
    "Ø®Ø° Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ØµØ¹ÙˆØ¯ ÙˆØ³Ù‚ÙˆØ· Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø±ÙˆÙ…Ø§Ù†ÙŠØ©ØŒ ÙˆØ§Ù„Ù…Ø§ÙŠØ§ØŒ ÙˆØ­ØªÙ‰ Ø§Ù„Ø³Ù„Ø§Ù„Ø§Øª Ø§Ù„Ù…ØµØ±ÙŠØ©. \n",
    "Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ Ø§Ù†Ù‡ÙŠØ§Ø±Ù‡Ø§ØŸ Ù‡Ù„ ÙƒØ§Ù† Ø§Ù„ÙØ³Ø§Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØŒ Ø£Ù… Ø§Ù„ØºØ²Ùˆ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØŒ Ø£Ù… Ù…Ø¬Ø±Ø¯ Ù…Ø±ÙˆØ± Ø§Ù„Ø²Ù…Ù†ØŸ \n",
    "Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø±ØŒ Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ ÙˆØ§Ø­Ø¯ Ù…Ø¤ÙƒØ¯: Ù„Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© ØªØ¯ÙˆÙ… Ù„Ù„Ø£Ø¨Ø¯!\"\"\"\n",
    "\n",
    "arabic_pipline_test = \"\"\"Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙØ­Ø¯Ø« ØªØºÙŠÙŠØ±Ø§Øª Ø¬Ø°Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙ†Ø§Ø¹Ø§ØªØ› \n",
    "Ø§Ù„Ø´Ø±ÙƒØ§Øª ØªØªØ¨Ù†Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ø£ØªÙ…ØªØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§ØªØŒ ÙˆØªØ­Ø³ÙŠÙ† ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡Ø› \n",
    "ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ØªØ¸Ù„ Ø§Ù„Ù…Ø®Ø§ÙˆÙ Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ© Ø¨Ø´Ø£Ù† Ø®ØµÙˆØµÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­ÙŠØ² Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠ.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "### Introduction\n",
    "Sentence segmentation is the process of dividing a text into meaningful sentences. It ensures that a passage is correctly split at appropriate sentence boundaries, which is essential for tasks like text summarization, machine translation, and speech processing.\n",
    "\n",
    "### Conclusion\n",
    "Sentence segmentation is a crucial step in Natural Language Processing (NLP) that transforms raw text into structured data by identifying and separating sentences. It plays a vital role in improving text analysis, enhancing model performance, and ensuring better readability. ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aakam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from spacy.language import Language\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology is evolving rapidly; every day, we witness new advancements in artificial intelligence, robotics, and data science. \n",
      "\n",
      "The question is: are we ready for such changes?\n",
      "Many experts believe that automation will replace millions of jobsâ€”some say this is a threat, while others see it as an opportunity! \n",
      "\n",
      "Nevertheless, innovation continues: new industries are emerging, and with them, new career paths are being created.\n"
     ]
    }
   ],
   "source": [
    "doc_1 = nlp(article_1)\n",
    "for sent in doc_1.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1[10].is_sent_end, doc_1[0].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why We Can't Index `doc.sents` Directly\n",
    "\n",
    "### Handling Sentence Segmentation in Code\n",
    "In **spaCy**, sentences are extracted using `doc.sents`, but it returns a generator, not a list. This means you **cannot use indexing** like `doc_1.sents[10]`. Instead, convert it to a list first:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc_1 = nlp(\"This is the first sentence. This is the second sentence.\")\n",
    "sentences = list(doc_1.sents)\n",
    "print(sentences[1].text)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Technology is evolving rapidly; every day, we witness new advancements in artificial intelligence, robotics, and data science. ,\n",
       " The question is: are we ready for such changes?,\n",
       " Many experts believe that automation will replace millions of jobsâ€”some say this is a threat, while others see it as an opportunity! ,\n",
       " Nevertheless, innovation continues: new industries are emerging, and with them, new career paths are being created.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [sent for sent in doc_1.sents]\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br/>\n",
    "<p align=\"center\">\n",
    "  <img src=\"../img/1.jpeg\" alt=\"nlp pipeline\" width=\"800\">\n",
    "</p>\n",
    "<Br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"set_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\":\n",
    "            doc[token.i + 1].is_sent_start = True  \n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_boundaries\", before=\"parser\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'set_boundaries',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence is rapidly transforming various industries;\n",
      "===========================================================================\n",
      "businesses are adopting machine learning models to automate processes, \n",
      "enhance customer experiences, and gain insights from data;\n",
      "===========================================================================\n",
      "however, ethical concerns regarding data privacy, algorithmic bias, and job displacement continue\n",
      "to be major topics of discussion in academic and professional circles worldwide.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_pipline_en = nlp(english_pipline_test)\n",
    "for sent in list(doc_pipline_en.sents):\n",
    "    print(f\"{sent}\")\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "### Difference Between Custom `PunktSentenceTokenizer` and `sent_tokenize()`\n",
    "\n",
    "the PunktSentenceTokenizer is an unsupervised sentence tokenize Then, it applies those learned rules to `article_2` This means that the sentence splitting behavior depends on how `article_1`\n",
    "\n",
    "sent_tokenize() uses the pre-trained Punkt tokenizer that comes with NLTK, It does not require training on any custom text, It applies general rules to split article_2 into sentences.\n",
    "\n",
    "```python\n",
    "custom_tokenizer = PunktSentenceTokenizer(article_1)  # Train tokenizer on article_1\n",
    "doc_3 = custom_tokenizer.tokenize(article_2)  # Tokenize article_2 using trained model\n",
    "```\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History has taught us valuable lessonsâ€”some we remember, others we repeat!\n",
      "===========================================================================\n",
      "Take, for example, the rise and fall of ancient civilizations: the Roman Empire, the Maya, and even the Egyptian dynasties.\n",
      "===========================================================================\n",
      "What led to their decline?\n",
      "===========================================================================\n",
      "Was it internal corruption, external invasion, or simply the passage of time?\n",
      "===========================================================================\n",
      "Regardless, one thing remains certain: no empire lasts forever!\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_2 = sent_tokenize(article_2)\n",
    "\n",
    "for s in doc_2:\n",
    "    print(s)\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History has taught us valuable lessonsâ€”some we remember, others we repeat!\n",
      "===========================================================================\n",
      "Take, for example, the rise and fall of ancient civilizations: the Roman Empire, the Maya, and even the Egyptian dynasties.\n",
      "===========================================================================\n",
      "What led to their decline?\n",
      "===========================================================================\n",
      "Was it internal corruption, external invasion, or simply the passage of time?\n",
      "===========================================================================\n",
      "Regardless, one thing remains certain: no empire lasts forever!\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer = PunktSentenceTokenizer(article_1)\n",
    "doc_3 = custom_tokenizer.tokenize(article_2)\n",
    "\n",
    "\n",
    "for s in doc_3:\n",
    "    print(s)\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¹Ù„Ù…ØªÙ†Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¯Ø±ÙˆØ³Ù‹Ø§ Ù‚ÙŠÙ‘Ù…Ø©â€”Ø¨Ø¹Ø¶Ù‡Ø§ Ù†ØªØ°ÙƒØ±Ù‡ØŒ ÙˆØ¨Ø¹Ø¶Ù‡Ø§ Ù†Ø¹ÙŠØ¯ ØªÙƒØ±Ø§Ø±Ù‡! \n",
      "\n",
      "Ø®Ø° Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ ØµØ¹ÙˆØ¯ ÙˆØ³Ù‚ÙˆØ· Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ø±ÙˆÙ…Ø§Ù†ÙŠØ©ØŒ ÙˆØ§Ù„Ù…Ø§ÙŠØ§ØŒ\n",
      "ÙˆØ­ØªÙ‰ Ø§Ù„Ø³Ù„Ø§Ù„Ø§Øª Ø§Ù„Ù…ØµØ±ÙŠØ©. \n",
      "\n",
      "Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ Ø§Ù†Ù‡ÙŠØ§Ø±Ù‡Ø§ØŸ Ù‡Ù„ ÙƒØ§Ù† Ø§Ù„ÙØ³Ø§Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØŒ Ø£Ù… Ø§Ù„ØºØ²Ùˆ Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØŒ Ø£Ù… Ù…Ø¬Ø±Ø¯ Ù…Ø±ÙˆØ± Ø§Ù„Ø²Ù…Ù†ØŸ \n",
      "Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø±ØŒ Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡\n",
      "ÙˆØ§Ø­Ø¯\n",
      "Ù…Ø¤ÙƒØ¯: Ù„Ø§ Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ±ÙŠØ© ØªØ¯ÙˆÙ… Ù„Ù„Ø£Ø¨Ø¯!\n"
     ]
    }
   ],
   "source": [
    "doc_4 = nlp(article_4)\n",
    "for sent in doc_4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙØ­Ø¯Ø« ØªØºÙŠÙŠØ±Ø§Øª Ø¬Ø°Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙ†Ø§Ø¹Ø§ØªØ› \n",
      "Ø§Ù„Ø´Ø±ÙƒØ§Øª ØªØªØ¨Ù†Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ø£ØªÙ…ØªØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§ØªØŒ ÙˆØªØ­Ø³ÙŠÙ† ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡Ø› \n",
      "\n",
      "===========================================================================\n",
      "ÙˆÙ…Ø¹ Ø°Ù„Ùƒ\n",
      "===========================================================================\n",
      "ØŒ\n",
      "===========================================================================\n",
      "ØªØ¸Ù„ Ø§Ù„Ù…Ø®Ø§ÙˆÙ Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ© Ø¨Ø´Ø£Ù† Ø®ØµÙˆØµÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­ÙŠØ² Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠ.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_pipline_ar = nlp(arabic_pipline_test)\n",
    "for sent in list(doc_pipline_ar.sents):\n",
    "    print(f\"{sent}\")\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ØªØªØ·ÙˆØ± Ø¨Ø³Ø±Ø¹Ø©Ø› Ù†Ø´Ù‡Ø¯ ÙƒÙ„ ÙŠÙˆÙ… ØªØ·ÙˆØ±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙˆØ§Ù„Ø±ÙˆØ¨ÙˆØªØ§ØªØŒ ÙˆØ¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n",
      "===========================================================================\n",
      "Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‡Ùˆ: Ù‡Ù„ Ù†Ø­Ù† Ù…Ø³ØªØ¹Ø¯ÙˆÙ† Ù„Ù‡Ø°Ù‡ Ø§Ù„ØªØºÙŠØ±Ø§ØªØŸ ÙŠØ¹ØªÙ‚Ø¯ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø®Ø¨Ø±Ø§Ø¡ Ø£Ù† Ø§Ù„Ø£ØªÙ…ØªØ© Ø³ØªØ­Ù„ Ù…Ø­Ù„ Ù…Ù„Ø§ÙŠÙŠÙ† Ø§Ù„ÙˆØ¸Ø§Ø¦Ùâ€”Ø§Ù„Ø¨Ø¹Ø¶ ÙŠØ±Ø§Ù‡Ø§ ØªÙ‡Ø¯ÙŠØ¯Ù‹Ø§ØŒ Ø¨ÙŠÙ†Ù…Ø§ ÙŠØ±Ø§Ù‡Ø§ Ø¢Ø®Ø±ÙˆÙ† ÙØ±ØµØ©!\n",
      "===========================================================================\n",
      "ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ØªØ³ØªÙ…Ø± Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª: ØªÙ†Ø´Ø£ ØµÙ†Ø§Ø¹Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŒ ÙˆÙ…Ø¹Ù‡Ø§ ØªÙØ®Ù„Ù‚ Ù…Ø³Ø§Ø±Ø§Øª ÙˆØ¸ÙŠÙÙŠØ© Ø­Ø¯ÙŠØ«Ø©.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_5 = sent_tokenize(article_3)\n",
    "\n",
    "for s in doc_5:\n",
    "    print(s)\n",
    "    print(\"===\"*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
