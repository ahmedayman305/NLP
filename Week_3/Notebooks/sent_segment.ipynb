{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Article 1\n",
    "article_1 = \"\"\"Technology is evolving rapidly; every day, we witness new advancements in artificial intelligence, robotics, and data science. \n",
    "The question is: are we ready for such changes? Many experts believe that automation will replace millions of jobs—some say this is a threat, while others see it as an opportunity! \n",
    "Nevertheless, innovation continues: new industries are emerging, and with them, new career paths are being created.\"\"\"\n",
    "\n",
    "# English Article 2\n",
    "article_2 = \"\"\"History has taught us valuable lessons—some we remember, others we repeat! \n",
    "Take, for example, the rise and fall of ancient civilizations: the Roman Empire, the Maya, and even the Egyptian dynasties. \n",
    "What led to their decline? Was it internal corruption, external invasion, or simply the passage of time? \n",
    "Regardless, one thing remains certain: no empire lasts forever!\"\"\"\n",
    "\n",
    "# English Article 3\n",
    "english_pipline_test = \"\"\"Artificial intelligence is rapidly transforming various industries; businesses are adopting machine learning models to automate processes, \n",
    "enhance customer experiences, and gain insights from data; however, ethical concerns regarding data privacy, algorithmic bias, and job displacement continue\n",
    "to be major topics of discussion in academic and professional circles worldwide.\"\"\"\n",
    "\n",
    "# Arabic Article 1\n",
    "article_3 = \"\"\"التكنولوجيا تتطور بسرعة؛ نشهد كل يوم تطورات جديدة في الذكاء الاصطناعي، والروبوتات، وعلم البيانات. \n",
    "السؤال هو: هل نحن مستعدون لهذه التغيرات؟ يعتقد العديد من الخبراء أن الأتمتة ستحل محل ملايين الوظائف—البعض يراها تهديدًا، بينما يراها آخرون فرصة! \n",
    "ومع ذلك، تستمر الابتكارات: تنشأ صناعات جديدة، ومعها تُخلق مسارات وظيفية حديثة.\"\"\"\n",
    "\n",
    "# Arabic Article 2\n",
    "article_4 = \"\"\"علمتنا التاريخ دروسًا قيّمة—بعضها نتذكره، وبعضها نعيد تكراره! \n",
    "خذ على سبيل المثال صعود وسقوط الحضارات القديمة: الإمبراطورية الرومانية، والمايا، وحتى السلالات المصرية. \n",
    "ما الذي أدى إلى انهيارها؟ هل كان الفساد الداخلي، أم الغزو الخارجي، أم مجرد مرور الزمن؟ \n",
    "بغض النظر، هناك شيء واحد مؤكد: لا إمبراطورية تدوم للأبد!\"\"\"\n",
    "\n",
    "arabic_pipline_test = \"\"\"الذكاء الاصطناعي يُحدث تغييرات جذرية في العديد من الصناعات؛ \n",
    "الشركات تتبنى نماذج التعلم الآلي لأتمتة العمليات، وتحسين تجارب العملاء؛ \n",
    "ومع ذلك، تظل المخاوف الأخلاقية بشأن خصوصية البيانات والتحيز الخوارزمي.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "### Introduction\n",
    "Sentence segmentation is the process of dividing a text into meaningful sentences. It ensures that a passage is correctly split at appropriate sentence boundaries, which is essential for tasks like text summarization, machine translation, and speech processing.\n",
    "\n",
    "### Conclusion\n",
    "Sentence segmentation is a crucial step in Natural Language Processing (NLP) that transforms raw text into structured data by identifying and separating sentences. It plays a vital role in improving text analysis, enhancing model performance, and ensuring better readability. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aakam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from spacy.language import Language\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology is evolving rapidly; every day, we witness new advancements in artificial intelligence, robotics, and data science. \n",
      "\n",
      "The question is: are we ready for such changes?\n",
      "Many experts believe that automation will replace millions of jobs—some say this is a threat, while others see it as an opportunity! \n",
      "\n",
      "Nevertheless, innovation continues: new industries are emerging, and with them, new career paths are being created.\n"
     ]
    }
   ],
   "source": [
    "doc_1 = nlp(article_1)\n",
    "for sent in doc_1.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1[10].is_sent_end, doc_1[0].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why We Can't Index `doc.sents` Directly\n",
    "\n",
    "### Handling Sentence Segmentation in Code\n",
    "In **spaCy**, sentences are extracted using `doc.sents`, but it returns a generator, not a list. This means you **cannot use indexing** like `doc_1.sents[10]`. Instead, convert it to a list first:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc_1 = nlp(\"This is the first sentence. This is the second sentence.\")\n",
    "sentences = list(doc_1.sents)\n",
    "print(sentences[1].text)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Technology is evolving rapidly; every day, we witness new advancements in artificial intelligence, robotics, and data science. ,\n",
       " The question is: are we ready for such changes?,\n",
       " Many experts believe that automation will replace millions of jobs—some say this is a threat, while others see it as an opportunity! ,\n",
       " Nevertheless, innovation continues: new industries are emerging, and with them, new career paths are being created.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [sent for sent in doc_1.sents]\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br/>\n",
    "<p align=\"center\">\n",
    "  <img src=\"../img/1.jpeg\" alt=\"nlp pipeline\" width=\"800\">\n",
    "</p>\n",
    "<Br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"set_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\":\n",
    "            doc[token.i + 1].is_sent_start = True  \n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_boundaries\", before=\"parser\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'set_boundaries',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence is rapidly transforming various industries;\n",
      "===========================================================================\n",
      "businesses are adopting machine learning models to automate processes, \n",
      "enhance customer experiences, and gain insights from data;\n",
      "===========================================================================\n",
      "however, ethical concerns regarding data privacy, algorithmic bias, and job displacement continue\n",
      "to be major topics of discussion in academic and professional circles worldwide.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_pipline_en = nlp(english_pipline_test)\n",
    "for sent in list(doc_pipline_en.sents):\n",
    "    print(f\"{sent}\")\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "### Difference Between Custom `PunktSentenceTokenizer` and `sent_tokenize()`\n",
    "\n",
    "the PunktSentenceTokenizer is an unsupervised sentence tokenize Then, it applies those learned rules to `article_2` This means that the sentence splitting behavior depends on how `article_1`\n",
    "\n",
    "sent_tokenize() uses the pre-trained Punkt tokenizer that comes with NLTK, It does not require training on any custom text, It applies general rules to split article_2 into sentences.\n",
    "\n",
    "```python\n",
    "custom_tokenizer = PunktSentenceTokenizer(article_1)  # Train tokenizer on article_1\n",
    "doc_3 = custom_tokenizer.tokenize(article_2)  # Tokenize article_2 using trained model\n",
    "```\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History has taught us valuable lessons—some we remember, others we repeat!\n",
      "===========================================================================\n",
      "Take, for example, the rise and fall of ancient civilizations: the Roman Empire, the Maya, and even the Egyptian dynasties.\n",
      "===========================================================================\n",
      "What led to their decline?\n",
      "===========================================================================\n",
      "Was it internal corruption, external invasion, or simply the passage of time?\n",
      "===========================================================================\n",
      "Regardless, one thing remains certain: no empire lasts forever!\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_2 = sent_tokenize(article_2)\n",
    "\n",
    "for s in doc_2:\n",
    "    print(s)\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History has taught us valuable lessons—some we remember, others we repeat!\n",
      "===========================================================================\n",
      "Take, for example, the rise and fall of ancient civilizations: the Roman Empire, the Maya, and even the Egyptian dynasties.\n",
      "===========================================================================\n",
      "What led to their decline?\n",
      "===========================================================================\n",
      "Was it internal corruption, external invasion, or simply the passage of time?\n",
      "===========================================================================\n",
      "Regardless, one thing remains certain: no empire lasts forever!\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer = PunktSentenceTokenizer(article_1)\n",
    "doc_3 = custom_tokenizer.tokenize(article_2)\n",
    "\n",
    "\n",
    "for s in doc_3:\n",
    "    print(s)\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "علمتنا التاريخ دروسًا قيّمة—بعضها نتذكره، وبعضها نعيد تكراره! \n",
      "\n",
      "خذ على سبيل المثال صعود وسقوط الحضارات القديمة: الإمبراطورية الرومانية، والمايا،\n",
      "وحتى السلالات المصرية. \n",
      "\n",
      "ما الذي أدى إلى انهيارها؟ هل كان الفساد الداخلي، أم الغزو الخارجي، أم مجرد مرور الزمن؟ \n",
      "بغض النظر، هناك شيء\n",
      "واحد\n",
      "مؤكد: لا إمبراطورية تدوم للأبد!\n"
     ]
    }
   ],
   "source": [
    "doc_4 = nlp(article_4)\n",
    "for sent in doc_4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الذكاء الاصطناعي يُحدث تغييرات جذرية في العديد من الصناعات؛ \n",
      "الشركات تتبنى نماذج التعلم الآلي لأتمتة العمليات، وتحسين تجارب العملاء؛ \n",
      "\n",
      "===========================================================================\n",
      "ومع ذلك\n",
      "===========================================================================\n",
      "،\n",
      "===========================================================================\n",
      "تظل المخاوف الأخلاقية بشأن خصوصية البيانات والتحيز الخوارزمي.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_pipline_ar = nlp(arabic_pipline_test)\n",
    "for sent in list(doc_pipline_ar.sents):\n",
    "    print(f\"{sent}\")\n",
    "    print(\"===\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التكنولوجيا تتطور بسرعة؛ نشهد كل يوم تطورات جديدة في الذكاء الاصطناعي، والروبوتات، وعلم البيانات.\n",
      "===========================================================================\n",
      "السؤال هو: هل نحن مستعدون لهذه التغيرات؟ يعتقد العديد من الخبراء أن الأتمتة ستحل محل ملايين الوظائف—البعض يراها تهديدًا، بينما يراها آخرون فرصة!\n",
      "===========================================================================\n",
      "ومع ذلك، تستمر الابتكارات: تنشأ صناعات جديدة، ومعها تُخلق مسارات وظيفية حديثة.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "doc_5 = sent_tokenize(article_3)\n",
    "\n",
    "for s in doc_5:\n",
    "    print(s)\n",
    "    print(\"===\"*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
